The Emotion Detection project is an AI-based system designed to analyze spoken content from audio or video files and determine the emotional state of the speaker. It utilizes OpenAI's Whisper model for converting speech to text and identifying the language used, which enables the system to work with multilingual inputs. Audio is preprocessed using Librosa, which extracts important acoustic features such as Mel-Frequency Cepstral Coefficients (MFCC) and pitch—features commonly associated with variations in human emotions. These features are then fed into a Support Vector Machine (SVM) classifier, which has been trained on a simulated dataset containing randomly generated features and labels for four basic emotions: happy, sad, angry, and neutral. The primary goal is to showcase a full pipeline that includes audio conversion, transcription, feature extraction, emotion classification, and result visualization. However, the current implementation has several limitations: it relies on synthetic data rather than real emotional speech, lacks real-time functionality, and doesn’t incorporate multimodal data such as facial expressions or text sentiment. It also doesn't evaluate model accuracy or adapt to personalized emotion patterns. Despite these constraints, the project serves as a strong foundation for further development. The future scope includes integrating real emotional datasets, enabling real-time analysis, expanding emotion categories (e.g., fear, surprise, disgust), analyzing the textual meaning of transcribed speech for better emotion context using Large Language Models (LLMs), and enhancing the user experience with a web or mobile interface. The long-term vision is to create an intelligent, multimodal system that can accurately understand human emotions and apply this understanding in areas like mental health support, smart assistants, education, and customer engagement.
